##-------------------------------------
## SFTP
##-------------------------------------

snippet download
	`r 
	if (file.exists('sftp-config.json')) {
		sftp_info <- jsonlite::fromJSON('sftp-config.json')
		suppressMessages(library(ssh))	
		## connect
		scp_session <- ssh_connect(host = paste(sftp_info$user,sftp_info$host,sep='@'), 
								   passwd = sftp_info$password)
		## get paths
		local_path <- rstudioapi::getActiveDocumentContext()$path
		remote_path <- file.path(sftp_info$remote_path,
							   gsub(paste0('.*',basename(sftp_info$remote_path),'/'), '', local_path))
		## upload 
		scp_upload(scp_session, files=local_path, to=remote_path)
		ssh_disconnect(scp_session)
		## log
		print(sprintf("# downloaded '%s' to '%s'",basename(local_path),remote_path))
	} else {
		print("# error: 'sftp-config.json' not found on local")
	}`

snippet upload
	`r 
	if (file.exists('sftp-config.json')) {
		sftp_info <- jsonlite::fromJSON('sftp-config.json')
		suppressMessages(library(ssh))
		## save
		# rstudioapi::documentSave(rstudioapi::getActiveDocumentContext()$id)
		## connect
		scp_session <- ssh_connect(host = paste(sftp_info$user,sftp_info$host,sep='@'), 
								   passwd = sftp_info$password)
		## get paths
		local_path <- rstudioapi::getActiveDocumentContext()$path
		remote_path <- file.path(sftp_info$remote_path,
							   gsub(paste0('.*',basename(sftp_info$remote_path),'/'), '', local_path))
		## upload 
		scp_upload(scp_session, files=local_path, to=remote_path)
		ssh_disconnect(scp_session)
		## log
		print(sprintf("# uploaded '%s' to '%s'",basename(local_path),remote_path))
	} else {
		print("# error: 'sftp-config.json' not found on local")
	}`

##-------------------------------------
## VIZ
##-------------------------------------

snippet gg corr grid
	require(ggplot2)
	require(magrittr)
	require(ggrepel)
	
	p <- df %>%
		group_by(facet_col_var, facet_row_var) %>%
		arrange(-(x_val + y_val)) %>%
		mutate(##label top 5 points in each quadrant
			   point_lab = ifelse(1:n() %in% 1:5, name, ''), 
			   ##label correlation in each quadrant
			   r_lab = ifelse(1:n() == 1, 
							  paste("rho ==",round(cor(frac_ads, frac_earned, use="pairwise.complete.obs"),2)),
							  "")) %T>%
		with({ ##adjust axes limits 
			p_lim_max <<- max(max(x_val, na.rm=T), max(y_val, na.rm=T))
		}) %>%
		ggplot(aes(x=x_val, y=y_val)) +
		geom_point(alpha=0.2) +
		geom_text_repel(aes(label=point_lab), size=2, alpha=0.8) + 
		geom_text(aes(label=rlab, x=0+(p_lim_max/10), y=95*p_lim_max/100)), parse=TRUE) +
		geom_segment(data = data.frame(placeholder=NA), 
					 aes(x=0, y=0, xend=p_lim_max, yend=p_lim_max), 
					 linetype="dashed", alpha=0.5) +
		facet_grid(type ~ Candidate_ID) + 
		scale_x_continuous(label=scales::percent_format(accuracy=1), limits=c(0,p_lim_max)) +
		scale_y_continuous(label=scales::percent_format(accuracy=1), limits=c(0,p_lim_max)) +
		theme_bw()

snippet gg grid shared legend
	require(ggplot2)
	    plist <- list()
	    plist[["plot1"]] <- ggplot(data, aes(x=x,y=y)) + 
	        geom_point() + 
	        theme_bw() +
	        theme(legend.position="none")
	    plist[["plot2"]] <- ggplot(data, aes(x=x,y=y)) +
	        geom_point() + 
	        theme_bw() +
	        theme(legend.position="none")
	
	    legend <-  cowplot::get_legend(
	        plist[[names(plist)[1]]] + theme(legend.position="bottom") ##or top
	    )
	    pgrid <- cowplot::plot_grid(plotlist = plist, ncol = 2, align="hv")
	    pgrid_with_legend <- cowplot::plot_grid(pgrid, legend, ncol=1, rel_heights = c(1, .25))

snippet gg time
	require(ggplot2)
	    require(zoo)
	
	    data <- data.frame(x=1:100, y=rnorm(100))
	    data <- data %>%
	        mutate(x=as.Date(x)) %>%
	        arrange(x) %>%
	        mutate(y=zoo::rollmean(y, 7, fill=NA)) ##7-day smoothing window
	
	    data %>%
	        ggplot(aes(x=x, y=y)) + 
	        geom_line() +
	        scale_x_date(date_labels = "%b %y") + 
	        scale_x_date(date_labels = "%b %d") +
	        theme_bw()

snippet gg hist
	require(ggplot2)
	
	h <- hist(df$var, breaks=100, plot=F)
	hdf <- data.frame(x=h$mids, y=h$counts/sum(h$counts))
	ggplot(hdf, aes(x=x, y=y)) + 
		geom_bar(stat="identity", color="black") +
		stat_smooth(formula= y ~ s(x, k=10), method="gam", se=FALSE, color="red", lwd=.5) +
		theme_classic()

snippet gg coefs
	## Visualize `lm()` results (try using `radon` data)
	require(ggplot2)
	require(tidyverse)
	require(broom)
	
	data <- radon
	
	mod <- lm(log(radon) ~ factor(county) + factor(first_floor), data)
	mod_df <- tidy(mod)
	
	mod_viz_df <- mod_df %>% 
		mutate(y=estimate, 
			ymin=estimate-1.96*std.error, 
			ymax=estimate+1.96*std.error, 
			model="OLS") %>%
		##>>>>factor format option 1
		# mutate(term=gsub("factor\\((\\w*)\\)(.*)","\\1 = \\2", term)) %>%
		##==========================
		mutate(term=gsub("factor\\((\\w*)\\)(.*)","\\2", term)) %>%
		##<<<<factor format option 2
		mutate(term=gsub("\\(Intercept\\)", "intercept", term)) %>%
		# mutate(term=gsub("_"," ", term)) %>%
		select(term, y, ymin, ymax, model)
	mod_viz_df %>%
		ggplot(aes(x=term, y=y, ymin=ymin, ymax=ymax)) +
		geom_point() + geom_linerange() + xlab("") + ylab("estimate")

snippet gg coefs dodge
	## Visualize regression fits with multiple alternative models (example uses radon data)
	require(ggplot2)
	require(broom)
	require(latex2exp)
	
	data <- radon
	
	mod1 <- lm(log(radon) ~ factor(county), data)
	mod2 <- lm(log(radon) ~ factor(county) + factor(first_floor), data)
	mod3 <- lm(log(radon) ~ factor(county) + factor(first_floor) + factor(blue_earth), data)
	
	summary(mod1)
	summary(mod2)
	summary(mod3)
	
	mod1_df <- tidy(mod1) %>% mutate(model="mod1")
	mod2_df <- tidy(mod2) %>% mutate(model="mod2")
	mod3_df <- tidy(mod3) %>% mutate(model="mod3")
	
	viz_df <- bind_rows(mod1_df, mod2_df, mod3_df) %>%
		mutate(sig=ifelse(estimate-1.96*std.error < 0 & estimate+1.96*std.error > 0, 
						  "gray", "black"))
	
	p <- ggplot(viz_df, aes(x=term)) +
		## null hypothesis
		geom_hline(yintercept=0, lty=2, alpha=0.5) +
		## 95% CI
		geom_pointrange(aes(y=estimate, 
							ymin=estimate-1.96*std.error, 
							ymax=estimate+1.96*std.error, 
							shape=model, colour=sig), 
						position=position_dodge(width=.75), 
						size=0.6, stroke=.5) +
		## 90% CI
		geom_linerange(aes(y=estimate, 
						   ymin=estimate-1.65*std.error, 
						   ymax=estimate+1.65*std.error, 
						   shape=model, colour=sig), 
					   position=position_dodge(width=.75), 
					   size=1, stroke=.5) +
		scale_colour_identity() + 
		scale_shape_manual(values=c(16,25,15)) + 
		# facet_grid(~ group) + ## different outcomes
		ylab(TeX("Estimated Effect of XX on YY ($\\beta$)")) + 
		xlab("") +
		coord_flip() + 
		theme_bw() +
		theme(axis.text.x = element_text(size=8),
			  strip.text = element_text(size = 15),
			  # axis.title.x = element_text(size=10),
			  plot.margin = margin(5, 0, 7, -5),
			  legend.text = element_text(size = 8),
			  legend.title = element_text(size = 10),
			  legend.spacing.y = unit(1.0, 'cm'),
			  legend.position="bottom") + 
		guides(shape=guide_legend(title="model:", nrow=1))

snippet gg coefs grid
	## Visualize regression fits on multiple categories of outcomes
	require(ggplot2)
	require(tidyverse)
	require(broom)
	
	fit_df <- data_df %>%
		group_by(outcome_category) %>%
		do(tidy(lm(outcome_val ~ gender + age + race + gender:race, data = .)))
	
	outcome_category_levels <- rev(sort(as.character(unique(fit_df$outcome_category))))
	fit_df$outcome_category <- factor(
		fit_df$outcome_category, levels=outcome_category_levels
	)
	
	viz_df <- fit_df %>%
		mutate(`p-value`=
			ifelse(p.value < 0.001, '0.001', 
				ifelse(p.value < 0.01, '0.01',
					ifelse(p.value < 0.05, '>0.01', '>0.01'))))
	
	viz_df["p-value"] <- factor(
		viz_df["p-value"], levels=c(">0.01", "0.01", "0.001")
	)
	
	## option 1 | rows: DVs, columns: IVs
	p <- viz_df %>%
		mutate(Term=
			ifelse(term == "racewhite", "Is White", 
				ifelse(term == "gendermale", "Is Male",
					ifelse(term == "gendermale:racewhite", "Is White:Is Male",
					"Age +10"
				)))) %>%
		mutate(Term=gsub("\\(Intercept\\)", "intercept", Term)) %>%
		ggplot(aes(x=outcome_category, y=estimate)) +
		facet_grid(. ~ Term, scales="free") +
		geom_point(aes(colour=`p-value`), size=1) +
		geom_hline(yintercept=0, lty=2) +
		geom_pointrange(aes(
			ymin=estimate-1.96*std.error, 
			ymax=estimate+1.96*std.error,
			colour=`p-value`
		)) +
		ylab("coefficient estimate") +
		coord_flip() + 
		theme_bw() +
		scale_colour_manual(values=c("#dddddd","#717171","#000000"))
	p
	
	## option 2 | rows: IVs, columns: DVs
	p <- viz_df %>%
		mutate(Term=
			ifelse(term == "racewhite", "Is White", 
				ifelse(term == "gendermale", "Is Male",
					ifelse(term == "gendermale:racewhite", "Is White:Is Male",
					"Age +10"
				)))) %>%
		mutate(Term=gsub("\\(Intercept\\)", "intercept", Term)) %>%
		ggplot(aes(x=Term, y=estimate)) +
		facet_grid(. ~ outcome_category, scales="free") +
		geom_point(aes(colour=`p-value`), size=1) +
		geom_hline(yintercept=0, lty=2) +
		geom_pointrange(aes(
			ymin=estimate-1.96*std.error, 
			ymax=estimate+1.96*std.error,
			colour=`p-value`
		)) +
		ylab("coefficient estimate") +
		theme_bw() +
		scale_colour_manual(values=c("#dddddd","#717171","#000000"))
	p

snippet gg barplot grid
	require(ggplot2)
	
	p <- ggplot(plot_df, aes(x=x, y=y, fill=fill_var)) +
		geom_bar(stat="identity", position="dodge", colour="black") +
		#scale_x_date(
		#    date_labels = "%b\n%Y"
		#) +
		facet_wrap(.~row_var, scales="free_x") +
		ylab("ylab") + 
		xlab("xlab") +
		scale_fill_manual(values = c("black", "grey")) + 
		theme_bw() +
		theme(legend.title=element_blank())
	p

snippet gg hist2
	require(ggplot2)
	
	plot_df <- rbind(
		cbind(hist_df1, distribution = "density 1"),
		cbind(hist_df2, distribution = "density 2")
	)
	
	p <- ggplot(plot_df) + 
		geom_density(aes(
			x=x, colour=distribution, fill=distribution
		), alpha=0.5) + 
		xlab("xlab") +
		scale_x_continuous(
			#breaks=c(10000,100000,1000000,10000000,100000000,1000000000), 
			#limits=c(1000,20000000000),
			#labels=c("$10,000","$100,000","$1 million","$10 million", "$100 million", "$1 billion"),
			trans="log1p"
		) +
		theme_bw() + 
		theme(axis.text.x = element_text(angle = 40, hjust = 1))
	p

##-------------------------------------
## LINALG
##-------------------------------------

snippet pca
	dat_scaled <- scale(dat, center = T, scale = T)
	
	pca_out <- prcomp(na.omit(dat_scaled))
	summary(pca_out) ## summarise principal components
	PC1 <- pca_out$x[,1]
	PC2 <- pca_out$x[,2]
	
	## viz data
	df <- data.frame(PC1, PC2)
	ggplot(df, aes(x=PC1, y=PC2)) + geom_point()
	
	## viz data with loadings
	pca_loads <- data.frame(var=rownames(pca_out$rotation), pca_out$rotation)
	pc_scale <- 10
	ggplot(df, aes(x=PC1, y=PC2)) + 
		geom_segment(data=pca_loads, 
					 aes(x=0, y=0, xend=(PC1*pc_scale), yend=(PC2*pc_scale)),
					 arrow=arrow(length = unit(1/2, "picas")), 
					 color = "black") +
		geom_point() + 
		annotate("text", 
				 x=(pca_loads$PC1*pc_scale), 
				 y=(pca_loads$PC2*pc_scale),
				 label=pca_loads$var) +
		geom_point()
	
	
	## summarise components 
	summary(pca_out)
	
	pca_out$rotation %>%
		as.data.frame() %>%
		rownames_to_column("var") %>%
		gather(key="PC", value="load", -var) %>%
		filter(PC %in% c("PC1")) %>%
		arrange(-load) %>%
		mutate(var=forcats::as_factor(var)) %>%
		ggplot(aes(x=var, y=load)) + 
		geom_bar(stat="identity") +
		facet_wrap(~PC) +
		coord_flip() +
		theme_bw()

##-------------------------------------
## ML
##-------------------------------------

snippet caret.training.metric
	## custom training metric in caret
	grid <- expand.grid(.mtry = c(1, 2, 10))
	
	sf <- function(data, lev = NULL, model = NULL, method = NULL) {
		mean_ape <- mean(abs(data$pred - data$obs)/data$obs)
		rmse <- sqrt(mean((data$pred - data$obs)^2))
		out <- c(-mean_ape, -rmse)
		names(out) <- c("Mean_APE", "RMSE")
		return(out)
	}
	tc <- caret::trainControl(method = "cv", 
							  number = 10,
							  summaryFunction = sf)
	
	fit <- caret::train(x = X_train, y = y_train,
						trControl = tc, 
						tuneGrid = grid,
						standardize = FALSE, 
						ntrees = 1000,
						maxit = 500, 
						method = "rf", ## example: random forest
						metric = "Mean_APE")

snippet lasso
	library(caret)
	library(glmnet) ##https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
	
	## regression with L-1 (abs value / "diamond"-shaped) regularization
	## - pro: encourages sparsity of non-zero coefficients
	## - con: more useful for variable selection than optimizing MSE
	## - con: when k > n, only n covariates selected
	## - con: for correlated covariates, picks one and discards others
	lasso_fit <- cv.glmnet(Xtrain, ytrain, family = "multinomial",
						   type.measure = "class")
	lasso_ypred <- predict(lasso_fit, newx = Xtest, type = "class")

snippet elasticnet
	library(caret)
	library(glmnet) ##https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
	
	## regression with mixture of L-1 (abs) and L-2 (squared)
	## regularization
	## - pro: can shrink correlated covariates, rather than pick one
	## - con: limited to regression framework
	elnet_grid = expand.grid(
		alpha = seq(0, 1, length = 11), ## 0=ridge; 1=lasso
		lambda = c(0, 1, 10, 100, 500, 1000) ##overall weight to penalty
	)
	elnet_tc = caret::trainControl(method = "repeatedcv", number = 10, repeats = 3)
	
	elnet_fit <- caret::train(x = Xtrain, y = ytrain,
		method = "glmnet", tuneGrid = elnet_grid, trControl = elnet_tc, 
		standardize = FALSE, maxit = 1000000, metric = "Accuracy")
	confusionMatrix(elnet_fit)

snippet svm
	library(caret)
	
	## finds hyperplanes between classes (y) using X
	## - pro: non-linear modeling 
	## - pro: regularization built in
	## - con: choice of kernel is non-trivial
	## - con: takes a long time
	svm_grid <- expand.grid(
		sigma = c(.01, .015, 0.2), ## radial kernel scaling constant
		C = c(0.75, 0.9, 1, 1.1, 1.25) ## regularization constant
	)
	svm_tc <- caret::trainControl(method = "cv", savePred=T,
		number = 10, repeats = 10, classProbs = TRUE, search = "random")
	
	svm_fit <- caret::train(x = Xtrain, y = ytrain,
		method = "svmRadial", ## svmLinear|svmRadial|svmPoly
		tuneGrid = svm_grid, 
		preProc = c("center","scale"), trControl = svm_tc, 
		standardize = FALSE, maxit = 1000000, metric = "Accuracy")
	confusionMatrix(svm_fit)

snippet randomforest
	library(caret)
	library(randomForest)
	
	## algorithmically grow "trees" to find splits in data and 'bag'
	## predictions 
	## - pro: no need to perform cross-validation over number of trees 
	rf_grid <- expand.grid(
		.mtry = c(1, 2, floor(ncol(X)/2), floor(sqrt(ncol(X))), ncol(X)-2, ncol(X)-1, ncol(X)) ## no. of variables used at each split
	)
	rf_tc <- caret::trainControl(method = "cv", savePred=T,
		number = 10, classProbs = TRUE)
	
	rf_fit <- caret::train(x = Xtrain, y = ytrain,
		method = "rf", tuneGrid = rf_grid, trControl = rf_tc, 
		ntree = 1500,
		standardize = FALSE, maxit = 1000000, metric = "Accuracy")
	confusionMatrix(rf_fit)

##-------------------------------------
## TEXT
##-------------------------------------

snippet stmfit
	K <- 10
	stm_proc <- stm::textProcessor(
		corpus_df, 
		metadata = subset(corpus_df, covars_list),
		customstopwords = quanteda::stopwords("en")
	)
	stm_prep <- stm::prepDocuments(
		stm_proc[["documents"]], 
		stm_proc[["vocab"]], 
		stm_proc[["meta"]]
	)
	stm_fit <- stm::stm(
		documents = stm_prep[["documents"]], 
		vocab = stm_prep[["vocab"]],
		K = K, 
		prevalence = NULL,
		max.em.its = 75, 
		data = stm_prep[["meta"]],
		init.type = "Spectral"
	)
	stm_lbls <- stm::labelTopics(stm_fit)

snippet textdict
	library(quanteda)
	dict <- dictionary(list(lab1=c("a","b","c")))   
	my_dfm <- dfm(c("My Christmas was ruined by your opposition tax plan.",
				   "Does the United_States or Sweden have more progressive taxation?"), remove = stopwords("english"))
	dfm_dict <- dfm_lookup(my_dfm, dict, 
						   valuetype = "regex", case_insensitive = TRUE)
	dfm_dict_df <- as.data.frame(dfm_dict)

snippet dfm
	library(quanteda)
	
	cat("\nmaking a corpus")
	my_corpus <- corpus(df,
		text_field = "text", docid_field = "id")
	cat("✓")
	
	cat("\nmaking tokens")
	my_tokens <- tokens(my_corpus, 
		remove_punct = TRUE,
		remove_symbols = TRUE,
		remove_numbers = TRUE,
		remove_url = TRUE) %>% 
		tokens_tolower() %>%
		tokens_wordstem() %>%
		tokens_select(min_nchar=4) %>%
		tokens_remove(pattern="REPLY|REPLIES") %>%
		tokens_remove(pattern=stopwords("en")) %>%
		tokens_ngrams(n=2)
	cat("✓")
	
	cat("\nmaking a DFM")
	my_dfm <- dfm(my_tokens,
		groups = "", ## optional
		tolower = TRUE) %>%
		dfm_trim(min_termfreq = 0.05, max_termfreq = 0.95, 
		   termfreq_type = "quantile")
	my_dfm_mat <- as.matrix(my_dfm)
	cat("✓") 
	
	## quick summaries
	tstat_freq <- textstat_frequency(my_dfm, n = 5)
	head(tstat_freq, 20)
	
	textplot_wordcloud(my_dfm)

##-------------------------------------
## LOGLIK
##-------------------------------------

snippet loglik lm
	lm_ll <- function(par, X, y) {
	  # m-clark.github.io/models-by-example
	  #
	  # par: parameters to be estimated
	  # X: predictor matrix with intercept column
	  # y: target
	  
	  # setup
	  beta   = par[-1]                             # coefficients
	  sigma2 = par[1]                              # error variance
	  sigma  = sqrt(sigma2)
	  N = nrow(X)
	  
	  # linear predictor
	  LP = X %*% beta                              # linear predictor
	  mu = LP                                      # identity link in the glm sense
	  
	  # calculate likelihood
	  ll = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood
	  # ll =  -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu)    # alternate log likelihood form
	
	  return(-sum(ll))                             # optim by default is minimization, and we want to maximize the likelihood 
	                                               # (see also fnscale in optim.control)
	}

snippet loglik logit
	logit_ll <- function(par, X, y) {
	  # m-clark.github.io/models-by-example
	  #
	  # par: parameters to be estimated
	  # X: predictor matrix with intercept column
	  # y: target
	  
	  # setup
	  beta = par                                # coefficients
	  N = nrow(X)
	  
	  # linear predictor
	  LP = X %*% beta                           # linear predictor
	  mu = plogis(LP)                           # logit link
	  
	  # calculate likelihood
	  ll = dbinom(y, size = 1, prob = mu, log = TRUE)         # log likelihood
	  #   ll =  y*log(mu) + (1 - y)*log(1-mu)   # alternate log likelihood form
	  
	  return(-sum(ll))                          # optim by default is minimization, and we want to maximize the likelihood 
	                                            # (see also fnscale in optim.control)
	}

snippet loglik hurdle
	hurdle_poisson_ll <- function(y, X, par) {
	  # m-clark.github.io/models-by-example
	  #
	  # par: parameters to be estimated
	  # X: predictor matrix with intercept column
	  # y: target
	
	  # setup
	  logitpars = par[grep('logit', names(par))]
	  poispars  = par[grep('pois', names(par))]
	  
	  # logit model part
	  Xlogit = X
	  ylogit = ifelse(y == 0, 0, 1)
	  
	  LPlogit = Xlogit %*% logitpars
	  mulogit = plogis(LPlogit)
	  
	  # calculate the likelihood
	  logliklogit = -sum( ylogit*log(mulogit) + (1 - ylogit)*log(1 - mulogit) )  
	  
	  # Poisson model part
	  Xpois = X[y > 0, ]
	  ypois = y[y > 0]
	  
	  mupois = exp(Xpois %*% poispars)
	  
	  # calculate the likelihood
	  loglik0    = -mupois
	  loglikpois = -sum(dpois(ypois, lambda = mupois, log = TRUE)) + 
	    sum(log(1 - exp(loglik0)))
	  
	  # combine likelihoods
	  loglik = loglikpois + logliklogit
	  
	  return(-loglik)
	}

##-------------------------------------
## GLM
##-------------------------------------

snippet se.robust
	library(lmtest)
	library(sandwich)
	
	fit <- lm(y ~ x + d, data = df)
	## Reminder:
	## - HC0 = (X'X)^-1X diag(e_i^2) X(X'X)^-1
	##		Classic Huber-White (1980) estimator to consistently 
	##		estimate Var(\hat\beta) with unknown heteroskedasticity.
	## - HC1 = N/(N-K) * HC0
	##		MacKinnon and White (1985) degrees of freedom inflation 
	##		to reduce bias in finite samples with unknown 
	##      heteroskedasticity.
	## - HC2 = X'X)^-1X diag(e_i^2/(1-h_ii)) X(X'X)^-1
	##		Gives extra weight to influential (leverage) observations
	##		to reduce bias in finite samples with unknown
	##		heteroskedasticity.
	## - HC2 = X'X)^-1X diag(e_i^2/(1-h_ii)^2) X(X'X)^-1
	##		Gives extra EXTRA weight to influential observations
	##		to reduce bias in finite samples with unknown
	##		heteroskedasticity. Shown by Long and Ervin (2000)
	##		to have best finite sample performance.
	## See: 
	## cran.r-project.org/web/packages/sandwich/vignettes/sandwich.pdf
	fit.se.robust <- coeftest(fit, vcov. = vcovHC(fit, type = "HC0"))

snippet se.clus
	library(lmtest)
	library(sandwich)
	
	fit <- lm(y ~ x + d, data = df)
	fit.se.clus <- coeftest(fit, vcov. = vcovCL, cluster = ~d))

snippet outliers
	fit <- lm(y ~ x + d, data = df)
	## outliers: far from mean of Y distribution, decrease model fit
	## 	- check residuals
	resid(fit)
	## 	- check jack-knife residuals (i.e. out-of-sample errors for each obs)
	rstudent(fit)

	## leverage: far from mean of X distribution, contribute highly to coefficients
	## 	- check for high projection matrix or "hat" values
	hatvalues(fit)

	## influence: both far from Y and X distributions
	##	- check outlier x leverage values
	rstudent(fit) * hatvalues(fit)
	##	- check cooks distances (normalized so that 1 is a high value)
	cooks.distance(fit)

snippet jackresid
	## plot fitted values vs. jack-knife residuals
	## (residual for each observation is error when it 
	## is omitted upon fitting model ... used as 
	## outlier test)
	plot(fitted(mod), rstudent(mod))

snippet quasipoisfit
	## separate parameter for variance, rather than assuming mean and variance equal
	fit <- glm(y ~ x0 + x1, data=df, family=quasipoisson)


snippet nbinomfit
	library(MASS)
	# response is a mixture of poissons
	fit <- glm.nb(y ~ x0 + x1, data=df, link="log")

snippet hurdlefit
	library(pscl)
	## DGP: 
	## 1.) hurdle distribution decides if response is zero or not
	## 2.) if the response is not zero truncated count distribution 
	##     decides the exact non-zero value
	## covariates for truncated count component model: x0, x1
	## covariates for hurdle component model: z0, z1
	fit <- hurdle(y ~ x0 + x1 | z0 + z1, data=df, dist="poisson", zerodist="binomial")

snippet zipfit
	library(pscl)
	## DGP:
	## 1.) zero inflation model decides if response is definitely zero or not
	## 2.) count model decides the response otherwise (still could be zero!)
	## covariates predicting normal responses (zero or non-zero): x0, x1
	## covariates predicting systematic zero responses: z0, z1
	fit <- zeroinfl(y ~ x0 + x1 | z0 + z1, data=df, dist="poisson")

snippet poisfit
	fit <- glm(y ~ x0 + x1, data=df, family=poisson(link="log"))

snippet logitfit
	fit <- glm(y ~ x0 + x1, data=df, family=binomial(link="logit"))

snippet probitfit
	fit <- glm(y ~ x0 + x1, data=df, family=binomial(link="probit"))

snippet logit
	log(p/(1-p))

snippet logitinv
	1/(1+exp(-a))
	exp(a)/(exp(a)+1)

snippet logit2prob
	exp(coef(fit)) / (1 + exp(coef(fit)))

snippet na.convert.mean
	na.convert.mean <- function (df) {
		vars <- names(df)
		if (!is.null(resp <- attr(attr(df, "terms"), "response"))) {
			vars <- vars[-resp]
			x <- df[[resp]]
			pos <- is.na(x)
			if (any(pos)) {
				df <- df[!pos, , drop = FALSE]
				warning(paste(sum(pos), "observations omitted due to missing values in the response"))
			}
		}
		for (j in vars) {  # j is variable names
			x <- df[[j]]
			pos <- is.na(x)
			if (any(pos)) {
				if (length(levels(x))) {   # factors
					xx <- as.character(x)
					xx[pos] <- "NA"
					x <- factor(xx, exclude = NULL)
				}
				else if (is.matrix(x)) {   # matrices
					ats <- attributes(x)
					x.na <- 1*pos
					w <- !pos
					n <- nrow(x)
					TT <- array(1, c(1, n))
					xbar <- (TT %*% x)/(TT %*% w)
					xbar <- t(TT) %*% xbar
					x[pos] <- xbar[pos]
					attributes(x) <- ats
					attributes(x.na) <- ats
					dimnames(x.na)[[2]]=paste(dimnames(x)[[2]],".na",sep='')
					df[[paste(j,".na",sep='')]] <- x.na 
				} else {   # ordinary numerical vector
					ats <- attributes(x)
					x[pos] <- mean(x[!pos])
					x.na <- 1*pos
					df[[paste(j,".na",sep='')]] <- x.na 
					attributes(x) <- ats
				}
				df[[j]] <- x
			}
		}
		df
	}

##-------------------------------------
## TOYDATA
##-------------------------------------

snippet radon
	radon <- "radon first_floor blue_earth clay goodhue
	1    0.9           1          0    1       0
	2    1.5           0          0    1       0
	3    1.8           0          1    0       0
	4    2.5           1          0    1       0
	5    2.6           0          0    1       0
	6    2.6           0          0    0       1
	7    3.5           0          0    0       1
	8    3.5           0          0    0       1
	9    3.5           1          0    1       0
	10   3.6           0          0    1       0
	11   3.8           0          1    0       0
	12   3.9           0          0    0       1
	13   4.7           0          1    0       0
	14   4.8           0          0    0       1
	15   4.9           0          0    0       1
	16   5.0           0          1    0       0
	17   5.6           0          0    0       1
	18   5.8           1          1    0       0
	19   6.0           0          1    0       0
	20   6.7           0          0    0       1
	21   6.8           0          1    0       0
	22   6.9           0          1    0       0
	23   6.9           1          0    1       0
	24   6.9           1          0    0       1
	25   7.2           0          1    0       0
	26   7.6           0          0    0       1
	27   8.8           0          0    1       0
	28   9.0           0          0    1       0
	29   9.5           0          1    0       0
	30   9.5           0          1    0       0
	31   9.8           1          0    0       1
	32  12.8           0          1    0       0
	33  12.9           0          0    1       0
	34  13.0           0          1    0       0
	35  13.0           0          0    1       0
	36  13.1           0          0    1       0
	37  14.3           0          0    0       1
	38  14.3           1          1    0       0
	39  19.5           0          0    1       0
	40  26.6           0          0    1       0
	41  43.5           0          0    0       1"
	radon_df <- read.table(text=radon)

##-------------------------------------
## DEBUGGING
##-------------------------------------

snippet mem
	format(object.size(X), 'GB')

snippet objectsize
	format(object.size(X), 'GB')

snippet timeit
	time_start <- Sys.time()
	print(time_start)
	${1}
	time_end <- Sys.time()
	time_taken <- time_end - time_start
	print(time_taken)

snippet tryc
	${1:variable} <- tryCatch({
		${2}
	}, warning = function(w) {
		message(sprintf("Warning in %s: %s", deparse(w[["call"]]), w[["message"]]))
		${3}
	}, error = function(e) {
		message(sprintf("Error in %s: %s", deparse(e[["call"]]), e[["message"]]))
		${4}
	}, finally = {
		${5}
	})

snippet parallel
	library(pbapply)
	
	nc <- parallel::detectCores()
	cl <- parallel::makeCluster(nc, outfile = "")
	doParallel::registerDoParallel(cl)
	
	n <- 10000
	out <- foreach(i = icount(n)) %dopar% {
		## my code
	}
	parallel::stopCluster(cl)
	
	## with progress bar
	iinvisible(parallel::clusterEvalQ(cl, { 
		cat("put preamble code for all workers here (e.g. loading libs)") 
	}))
	parallel::clusterExport(cl = cl, varlist = c("myvar", "myfunc"))
	out <- pblapply(cl = cl, X = 1:n, function(x) {
		## my code (using myvar and myfunc)
	})
	parallel::stopCluster(cl)

snippet pbar
	pbar <- utils::txtProgressBar(min=0, max=N, style=3)
	for (i in 1:N) {
		utils::setTxtProgressBar(pbar, i)
	}
	close(pbar)

snippet rmd
	xaringan::infinite_moon_reader(rstudioapi::getSourceEditorContext()[["path"]])

##-------------------------------------
## RMD
##-------------------------------------

snippet rmdheader
	---
	title: "Replication Notes for Discovery, Description, and Targetting of Anti-Vax Tweeters"
	author: Soubhik Barari
	date: "Last Compiled: \\today"
	output:
	  pdf_document:
		toc: true
		toc_depth: 3
		number_sections: true
	urlcolor: red
	fontsize: 12pt
	---

##-------------------------------------
## AESTHETICS
##-------------------------------------

snippet options
	options(readr.show_col_types = FALSE)

snippet header
	# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
	# Description of script.${1}
	# 
	# Author: Soubhik Barari
	# 
	# Environment:
	# - must use R 3.6
	# - must run on RCE
	# 
	# Runtime:
	# - Xh for ~y lines of inputs on May 28, 2018
	# 
	# Input:
	# - input.csv
	#
	# Output:
	# - output on xxx.harvard.edu
	# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

snippet section
	#####------------------------------------------------------#
	##### ${0} ####
	#####------------------------------------------------------#

##-------------------------------------
## HYPOTEST
##-------------------------------------

snippet equivtest
	data(ufc)
	### Tost
	tost(ufc$Height.m.p, ufc$Height.m, epsilon = 1)
	### equivalence plot
	ufc.ht <- ufc[!is.na(ufc$Height),]
	equivalence.xyplot(ufc.ht$Height.m ~ ufc.ht$Height.m.p,
	alpha=0.05, b0.ii=0.1, b1.ii=0.2,
	xlab="Predicted height (m)",
	ylab="Measured height (m)")

snippet bhq
	## BHq
	## adjust for multiple testing in a less harsh way 
	## than Bonferonni; cap E[false discovery rate] rather than 
	## Pr(falsely reject at least one null) with Holm
	
	coefs_df <- coefs_df[order(coefs_df$p.value),] ##stacked broomed models
	alpha <- 0.05 ##original threshold
	k <- nrow(coefs_df) ##number of hypotheses 
	r <- 1:k ##ranks of p-values
	alpha_adj <- (r*alpha)/k ##step-up 'tailored' thresholds
	sig_BH <- coefs_df$p.value < alpha_adj ##step-up hypothesis tests 
	z_crit_adj <- qnorm(1 - (alpha_adj)/2) ##new critical values for CIs (asymptotic)
	t_crit_adj <- qt(1 - (alpha_adj)/2, n-1) ##new critical values for CIs (if `n` is available)

##-------------------------------------
## PROB
##-------------------------------------

snippet lotv
	library(tidyverse)
	## Law of Total Variance
	## V[X] = {between group var.} + {within group var.}
	##      = V[E[X|Y]]            + E[V[X|Y]]
	df %>%
		filter(!is.na(X)) %>%
		mutate(
			tot_var=var(X),
			tot_mean=mean(Y),
			n=n()
		) %>%
		group_by(Y) %>%
		summarize(
			group_var=sum((X-mean(X))^2),
			group_mean=mean(X),
			group_n=n(),
			tot_var=head(tot_var, 1),
			tot_mean=head(tot_mean, 1),
			n=head(n, 1)
		) %>%
		summarize(
			tot_var=head(tot_var, 1),
			within_var=sum(group_var)/head(n, 1),
			between_var=sum(group_n*(group_mean-tot_mean)^2)/head(n, 1)
		)
	)

##-------------------------------------
## SURVEYS
##-------------------------------------

snippet qualtrics
	library(qualtRics)
	
	## can find API key in Qualtrics > Account Settings > Qualtrics ID
	qualtrics_api_key <- "GTY6dlP9xnuMaaXUq364fHJkPFIZAKbLn6jYHRuN"
	qualtrics_api_credentials(api_key = qualtrics_api_key,
							  base_url = "harvard.az1.qualtrics.com")
	
	surv_id <- "SV_8wA0COb8ufL6Xf7"
	surv <- fetch_survey(surveyID = surv_id,
						 force_request = TRUE,
						 verbose = TRUE)
	surv_qs <- survey_questions(surv_id)

##-------------------------------------
## IO
##-------------------------------------

snippet jsonfastread
	d <- RcppSimdJson::fload(fpath)

snippet csvfastread
	d <- DT::fread(fpath)
	d <- vroom::vroom(fpath)

##-------------------------------------
## OTHER
##-------------------------------------

snippet shinyapp
	library(shiny)
	
	ui <- fluidPage(
	  ${0}
	)
	
	server <- function(input, output, session) {
	  
	}
	
	shinyApp(ui, server)

snippet $$
	`r eval(parse(text = "system('$$', intern = TRUE)"))`

snippet $$$
	`r paste("$$$ <-", deparse(eval(parse(text="$$$")), width.cutoff = 500L))`

snippet optparse
	library(optparse)
	arg_list <- list(     
		make_option(c("--NAME"), type="character", default="default", 
			help="Name of model run for outputs",
			metavar="K")
	)
	ARGS <- parse_args(OptionParser(option_list=arg_list))

snippet args
	args = commandArgs(trailingOnly=TRUE)
	print(args[1])

snippet lib
	library(${1:package})

snippet req
	require(${1:package})

snippet src
	source("${1:file.R}")

snippet ret
	return(${1:code})

snippet mat
	matrix(${1:data}, nrow = ${2:rows}, ncol = ${3:cols})

snippet sg
	setGeneric("${1:generic}", function(${2:x, ...}) {
		standardGeneric("${1:generic}")
	})

snippet sm
	setMethod("${1:generic}", ${2:class}, function(${2:x, ...}) {
		${0}
	})

snippet sc
	setClass("${1:Class}", slots = c(${2:name = "type"}))

snippet if
	if (${1:condition}) {
		${0}
	}

snippet el
	else {
		${0}
	}

snippet ei
	else if (${1:condition}) {
		${0}
	}

snippet fun
	${1:name} <- function(${2:variables}) {
		${0}
	}

snippet for
	for (${1:variable} in ${2:vector}) {
		${0}
	}

snippet while
	while (${1:condition}) {
		${0}
	}

snippet switch
	switch (${1:object},
		${2:case} = ${3:action}
	)

snippet chunk
	chunk_size <- 100
	X <- 1:10000
	split(X, ceiling(seq_along(X)/chunk_size))

snippet apply
	apply(${1:array}, ${2:margin}, ${3:...})

snippet lapply
	lapply(${1:list}, ${2:function})

snippet checklist
	cat("\nItem 1"); $1; cat("✓")

snippet setwd.current
	setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

snippet checkmark
	✓

snippet sapply
	sapply(${1:list}, ${2:function})

snippet mapply
	mapply(${1:function}, ${2:...})

snippet tapply
	tapply(${1:vector}, ${2:index}, ${3:function})

snippet vapply
	vapply(${1:list}, ${2:function}, FUN.VALUE = ${3:type}, ${4:...})

snippet rapply
	rapply(${1:list}, ${2:function})

snippet ts
	`r paste("#", date(), "------------------------------\n")`